{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3407cc60-89d2-4f3a-afd2-30b75619ddf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/courses/IE7500.202530/shared/conda_env_1/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ Tokenizing full‚Äêthread prompts ‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 44847 examples [00:00, 203315.44 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44847/44847 [00:14<00:00, 3168.98 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44847/44847 [00:01<00:00, 40465.32 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Saved 44847 examples to 'tokenized_full'\n",
      "üí¨ Tokenizing subject+last‚Äêemail prompts ‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 44847 examples [00:00, 312236.09 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44847/44847 [00:13<00:00, 3302.28 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44847/44847 [00:01<00:00, 40529.87 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Saved 44847 examples to 'tokenized_subject'\n",
      "‚úÖ Step 3 complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# data_prep_step3.py\n",
    "\n",
    "import os\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def tokenize_and_save(input_jsonl: str, output_dir: str, max_len: int = 512):\n",
    "    \"\"\"\n",
    "    1) Loads the JSONL at `input_jsonl` (expects one {\"text\": ...} per line).\n",
    "    2) Tokenizes to GPT-2 input_ids (truncates/pads to max_len).\n",
    "    3) Sets labels = input_ids for causal LM.\n",
    "    4) Saves the processed dataset to `output_dir` (creates it if needed).\n",
    "    \"\"\"\n",
    "    # a) Load the prompts as a Hugging Face Dataset\n",
    "    ds = load_dataset(\"json\", data_files=input_jsonl, split=\"train\")\n",
    "\n",
    "    # b) Load GPT-2 tokenizer and set pad token\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # c) Tokenization function (batched)\n",
    "    def tokenize_fn(batch):\n",
    "        enc = tokenizer(\n",
    "            batch[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=max_len,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "        # Use the inputs themselves as labels for next‚Äêtoken prediction\n",
    "        enc[\"labels\"] = enc[\"input_ids\"].copy()\n",
    "        return enc\n",
    "\n",
    "    # d) Apply it\n",
    "    tokenized = ds.map(\n",
    "        tokenize_fn,\n",
    "        batched=True,\n",
    "        remove_columns=[\"text\"]\n",
    "    )\n",
    "\n",
    "    # e) Save to disk\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    tokenized.save_to_disk(output_dir)\n",
    "    print(f\"> Saved {len(tokenized)} examples to '{output_dir}'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Filepaths from Step 2\n",
    "    full_in  = \"enron_prompts_full.jsonl\"\n",
    "    subj_in  = \"enron_prompts_subject.jsonl\"\n",
    "\n",
    "    # Output folders for Step 4‚Äôs training\n",
    "    full_out = \"tokenized_full\"\n",
    "    subj_out = \"tokenized_subject\"\n",
    "\n",
    "    print(\"üí¨ Tokenizing full‚Äêthread prompts ‚Ä¶\")\n",
    "    tokenize_and_save(full_in, full_out)\n",
    "\n",
    "    print(\"üí¨ Tokenizing subject+last‚Äêemail prompts ‚Ä¶\")\n",
    "    tokenize_and_save(subj_in, subj_out)\n",
    "\n",
    "    print(\"‚úÖ Step 3 complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2436be0-a62b-446e-8298-4926fef60bea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
