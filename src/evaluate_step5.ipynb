{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702bf732-e27c-48e1-9629-79da0d20de06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# evaluate_step5.py\n",
    "#Load your fine-tuned GPT-2+LoRA model and tokenizer.\n",
    "#Generate replies for each example using your chosen prompt format.\n",
    "#Compute automatic metrics:\n",
    "#Save a JSON report of all metrics.\n",
    "\n",
    "import json\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_dataset\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bertscore\n",
    "\n",
    "\n",
    "def build_prompt(ex, fmt: str):\n",
    "    \"\"\"Construct the generation prompt (without the gold reply).\"\"\"\n",
    "    tone = ex[\"tone\"]\n",
    "    if fmt == \"full_thread\":\n",
    "        return f\"{tone} Thread: {ex['thread']} Reply:\"\n",
    "    else:\n",
    "        return (f\"{tone} Subject: {ex['subject']} â”ƒ Last message: \"\n",
    "                f\"{ex['email']} Reply:\")\n",
    "\n",
    "\n",
    "def compute_perplexity(model, tokenizer, texts, device):\n",
    "    \"\"\"Compute perplexity over a list of full <prompt + ' ' + reply> strings.\"\"\"\n",
    "    encodings = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**encodings, labels=encodings[\"input_ids\"])\n",
    "        neg_log_likelihood = outputs.loss * encodings[\"input_ids\"].size(1)\n",
    "    ppl = torch.exp(outputs.loss).item()\n",
    "    return ppl\n",
    "\n",
    "\n",
    "def main():\n",
    "    import argparse\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument(\"--model_dir\",   required=True,\n",
    "                   help=\"Path to fine-tuned model (e.g. outputs/gpt2_lora_full)\")\n",
    "    p.add_argument(\"--pairs_jsonl\", default=\"enron_pairs.jsonl\")\n",
    "    p.add_argument(\"--prompt_format\", choices=[\"full_thread\", \"subject_last_email\"],\n",
    "                   default=\"full_thread\")\n",
    "    p.add_argument(\"--batch_size\", type=int, default=16)\n",
    "    p.add_argument(\"--max_gen_length\", type=int, default=128)\n",
    "    args = p.parse_args()\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # 1) Load model & tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_dir)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model = AutoModelForCausalLM.from_pretrained(args.model_dir).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # 2) Read pairs JSONL\n",
    "    raw = [json.loads(l) for l in open(args.pairs_jsonl, encoding=\"utf-8\")]\n",
    "\n",
    "    # 3) Split off a validation slice (e.g., last 10%)\n",
    "    n = len(raw)\n",
    "    val = raw[int(0.9 * n):]\n",
    "\n",
    "    # 4) Generate replies and collect references\n",
    "    hyps, refs, ppl_texts = [], [], []\n",
    "    for ex in tqdm(val, desc=\"Generating\"):\n",
    "        prompt = build_prompt(ex, args.prompt_format)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            out_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_length=args.max_gen_length,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                do_sample=False,\n",
    "            )\n",
    "        gen = tokenizer.decode(out_ids[0][inputs.input_ids.size(1):], skip_special_tokens=True)\n",
    "        hyps.append(gen.strip().split())\n",
    "        refs.append([ex[\"reply\"].strip().split()])\n",
    "        ppl_texts.append(prompt + \" \" + ex[\"reply\"].strip())\n",
    "\n",
    "    # 5) Compute Perplexity\n",
    "    print(\"ðŸ“Š Computing perplexityâ€¦\")\n",
    "    perplexity = compute_perplexity(model, tokenizer, ppl_texts, device)\n",
    "\n",
    "    # 6) BLEU\n",
    "    print(\"ðŸ“Š Computing BLEUâ€¦\")\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    bleu = corpus_bleu(refs, hyps, smoothing_function=smoothie)\n",
    "\n",
    "    # 7) ROUGE-L\n",
    "    print(\"ðŸ“Š Computing ROUGE-Lâ€¦\")\n",
    "    scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=True)\n",
    "    rouge_l_scores = [scorer.score(\" \".join(r[0]), \" \".join(h))[\"rougeL\"].fmeasure\n",
    "                      for h, r in zip(hyps, refs)]\n",
    "    rouge_l = sum(rouge_l_scores) / len(rouge_l_scores)\n",
    "\n",
    "    # 8) BERTScore\n",
    "    print(\"ðŸ“Š Computing BERTScoreâ€¦\")\n",
    "    # We need untokenized strings again:\n",
    "    hyp_strs = [\" \".join(h) for h in hyps]\n",
    "    ref_strs = [\" \".join(r[0]) for r in refs]\n",
    "    P, R, F1 = bertscore(hyp_strs, ref_strs, lang=\"en\", rescale_with_baseline=True)\n",
    "    bert_p, bert_r, bert_f1 = P.mean().item(), R.mean().item(), F1.mean().item()\n",
    "\n",
    "    # 9) Report\n",
    "    report = {\n",
    "        \"perplexity\": perplexity,\n",
    "        \"bleu\": bleu,\n",
    "        \"rouge_l\": rouge_l,\n",
    "        \"bertscore\": {\"precision\": bert_p, \"recall\": bert_r, \"f1\": bert_f1},\n",
    "        \"n_examples\": len(val),\n",
    "    }\n",
    "\n",
    "    print(\"\\n=== Evaluation Results ===\")\n",
    "    print(json.dumps(report, indent=2))\n",
    "    with open(\"evaluation_report.json\", \"w\") as fout:\n",
    "        json.dump(report, fout, indent=2)\n",
    "    print(\"Saved report â†’ evaluation_report.json\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
